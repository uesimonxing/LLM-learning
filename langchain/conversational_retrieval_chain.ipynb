{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{\n",
    "RunnableMap_{Output := Dict[str, Any]}^{Iutput} \\rightarrow\n",
    "BasePromptTemplate_{LanguageModelOutput := PromptValue}^{LanguageModelInput := Dict}\n",
    "\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = ChatPromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import format_document\n",
    "from operator import itemgetter\n",
    "\n",
    "# Create the retriever\n",
    "vectorstore = Chroma.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "def _combine_documents(docs, document_prompt = DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"Human: \" + dialogue_turn[0]\n",
    "        ai = \"Assistant: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "_inputs = RunnableMap(\n",
    "    {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: _format_chat_history(x['chat_history'])\n",
    "        } | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(),\n",
    "    }\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableSequence(first=RunnableMap(steps={'standalone_question': RunnableSequence(first=RunnableMap(steps={'question': <langchain.schema.runnable.RunnableLambda object at 0x12afce0d0>, 'chat_history': <langchain.schema.runnable.RunnableLambda object at 0x12a76a290>}), middle=[ChatPromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:', template_format='f-string', validate_template=True), additional_kwargs={})]), ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='org-9Nwf09ogypSXZYMXF1fSW2si', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)], last=StrOutputParser())}), middle=[RunnableMap(steps={'context': RunnableSequence(first=<langchain.schema.runnable.RunnableLambda object at 0x12af98550>, middle=[VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x12a6c4690>, search_type='similarity', search_kwargs={})], last=<langchain.schema.runnable.RunnableLambda object at 0x12a6c5490>), 'question': <langchain.schema.runnable.RunnableLambda object at 0x12b7d1f90>}), ChatPromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), additional_kwargs={})])], last=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='org-9Nwf09ogypSXZYMXF1fSW2si', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain\n",
    "RunnableSequence(first=RunnableMap(steps={'standalone_question': RunnableSequence(first=RunnableMap(steps={'question': <langchain.schema.runnable.RunnableLambda object at 0x12afce0d0>, 'chat_history': <langchain.schema.runnable.RunnableLambda object at 0x12a76a290>}), middle=[ChatPromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:', template_format='f-string', validate_template=True), additional_kwargs={})]), ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='org-9Nwf09ogypSXZYMXF1fSW2si', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)], last=StrOutputParser())}), middle=[RunnableMap(steps={'context': RunnableSequence(first=<langchain.schema.runnable.RunnableLambda object at 0x12af98550>, middle=[VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x12a6c4690>, search_type='similarity', search_kwargs={})], last=<langchain.schema.runnable.RunnableLambda object at 0x12a6c5490>), 'question': <langchain.schema.runnable.RunnableLambda object at 0x12b7d1f90>}), ChatPromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), additional_kwargs={})])], last=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='org-9Nwf09ogypSXZYMXF1fSW2si', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None))\n",
    "\n",
    "\n",
    "# conversational_qa_chain.invoke({\n",
    "#    \"question\": \"where did harrison work?\",\n",
    "#    \"chat_history\": [],\n",
    "# })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-learning-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
