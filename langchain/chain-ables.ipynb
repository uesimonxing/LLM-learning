{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from langchain.load.serializable import Serializable\n",
    "\n",
    "from langchain.schema.runnable import *\n",
    "\n",
    "from langchain.schema import *\n",
    "from langchain.schema.language_model import *\n",
    "from langchain.llms import *\n",
    "from langchain.chat_models import *\n",
    "\n",
    "from langchain.prompts import *\n",
    "\n",
    "from langchain.output_parsers import *\n",
    "\n",
    "from langchain.retrievers import *\n",
    "\n",
    "import json\n",
    "\n",
    "def prettify_json(s: Serializable):\n",
    "    data = json.dumps(s.to_json(), indent=4, ensure_ascii=False)\n",
    "    print(f'{s.__class__}: {data}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textcolor{yellow}{R_{Output}^{Input}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\\\ $\n",
    "$\n",
    "\\_coerce\\_to\\_runnable := \n",
    "    \\left\\{ \n",
    "        \\begin{array}{l} \n",
    "            \\textcolor{yellow} {R_{Output}^{Input}} \\\\\n",
    "            \\textcolor{yellow} {C_{Output}^{[Input]}} \\\\ \n",
    "            \\textcolor{yellow} {\\text{M := str} \\rightarrow \n",
    "                \\left\\{\n",
    "                    \\begin{array}{l}\n",
    "                        {R_{Output}^{Input}} \\\\\n",
    "                        {C_{Output}^{[Input]}}\n",
    "                     \\end{array}\n",
    "                \\right.}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RunnableWithFallbacks_{Output}^{Iutput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RunnableSequence_{Output}^{Iutput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RunnableMap_{Dict := str \\rightarrow Any}^{Iutput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RunnableLambda_{Output}^{Iutput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RunnablePassthrougth_{Input}^{Iutput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RunnableBinding_{Output}^{Iutput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow}{RouterRunnable_{Output}^{RouterInput := TypedDict\\text{\\{key:str, input:Any\\}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**复合运算**\n",
    "\n",
    "$\n",
    "\\_coerce\\_to\\_runnable := \n",
    "    \\left\\{ \n",
    "        \\begin{array}{l} \n",
    "            \\textcolor{yellow} {R_{Output}^{Input}} \\\\\n",
    "            \\textcolor{yellow} {C_{Output}^{[Input]}} \\\\ \n",
    "            \\textcolor{yellow} {\\text{M := str} \\rightarrow \n",
    "                \\left\\{\n",
    "                    \\begin{array}{l}\n",
    "                        {R_{Output}^{Input}} \\\\\n",
    "                        {C_{Output}^{[Input]}}\n",
    "                     \\end{array}\n",
    "                \\right.}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\xrightarrow{\\_\\_or\\_\\_}\n",
    "\\textcolor{yellow}{RunnableSequence_{Output}^{Iutput}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textcolor{yellow}{R_{Output}^{Input}} \\xrightarrow{bind(**args:Any)} \\textcolor{yellow}{RunnableBinding_{Output}^{Iutput}} \\text { , with kwargs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textcolor{yellow}{R_{Output}^{Input}} \\xrightarrow{with\\_fallbacks(Sequence[Runnable[Input, Output]])} \\textcolor{yellow}{RunnableWithFallbacks_{Output}^{Iutput}} \\text { , with fallbacks}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runnable\n",
    "    - RunnableWithFallbacks\n",
    "    - RunnableSequence\n",
    "    - RunnableMap\n",
    "    - RunnableLambda\n",
    "    - RunnablePassthrough\n",
    "    - RouterRunnable\n",
    "\n",
    "**运行**\n",
    "\n",
    "| Runnable | Transition | Input | Output | Additional Arguments|\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Row 1, Col 1 | `invoke` | `Input` | `Output` | `config: Optional[RunnableConfig]` |\n",
    "| Row 1, Col 1 | `batch` | `inputs: List[Input]` | `List[Output]` | ` config: Optional[Union[RunnableConfig, List[RunnableConfig]]], *, max_concurrency: Optional[int]` |\n",
    "| Row 1, Col 1 | `stream` | `Input` | `Iterator[Output]` | `config: Optional[RunnableConfig]` |\n",
    "\n",
    "**复合运算**\n",
    "\n",
    "| Runnable | Transition | Rx | Ry | Additional Arguments|\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Row 1, Col 1 | `__or__` | `other: _coerce_to_runnable` | `RunnableSequence[Input, Other]` |  |\n",
    "| Row 1, Col 1 | `__xor__` | `other: _coerce_to_runnable` | `RunnableSequence[Other, Output]` |  |\n",
    "| Row 1, Col 1 | `bind` | `**kwargs: Any` | `RunnableBinding(bound=self, kwargs=kwargs)` |  |\n",
    "| Row 1, Col 1 | `with_fallbacks` | `fallbacks: Sequence[Runnable[Input, Output]]` | `RunnableWithFallbacks[Input, Output]` | `*, exceptions_to_handle: Tuple[Type[BaseException]] = (Exception,),` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BaseLanguageModel\n",
    "    - BaseLLM\n",
    "    - BaseChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "$BaseLanguageModel_{LanguageModelOutput}^{LanguageModelInput} := \\textcolor{yellow}{BaseLLM_{LanguageModelOutput := str}^{LanguageModelInput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BaseLLM\n",
    "    - BaseOpenAI\n",
    "        - OpenAI\n",
    "        - AzureOpenAI\n",
    "\n",
    "**运行**\n",
    "\n",
    "| BaseLLM | Transition | Input | Output | Additional Arguments|\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Row 1, Col 1 | `invoke` | `LanguageModelInput` | `str` | `config: Optional[RunnableConfig], *, stop: Optional[List[str]], **kwargs: Any` |\n",
    "| Row 1, Col 1 | `generate` | `prompts: List[str]` | `LLMResult` | `stop: Optional[List[str]], callbacks: Optional[Union[Callbacks, List[Callbacks]]], *, **kwargs: Any` |\n",
    "| Row 1, Col 1 | `__call__` | `prompt: str` | `str` | `stop: Optional[List[str]], callbacks: Optional[Union[Callbacks, List[Callbacks]]], *, **kwargs: Any` |\n",
    "| Row 1, Col 1 | `predict` | `text: str` | `str` | `*, stop: Optional[Sequence[str]], **kwargs: Any` |\n",
    "| Row 1, Col 1 | `predict_messages` | `messages: List[BaseMessage]` | `BaseMessage` | `*, stop: Optional[Sequence[str]], **kwargs: Any` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$BaseLanguageModel_{LanguageModelOutput}^{LanguageModelInput} \n",
    ":= \\textcolor{yellow} {BaseChatModel_{BaseMessageChunk}^{LanguageModelInput}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BaseChatModel\n",
    "    - ChatOpenAI\n",
    "        - AzureChatOpenAI\n",
    "        - PromptLayerChatOpenAI\n",
    "        - ChatAnyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**运行**\n",
    "\n",
    "| BaseChatModel | Transition | Input | Output | Additional Arguments|\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Row 1, Col 1 | `invoke` | `LanguageModelInput` | `BaseMessageChunk` | `config: Optional[RunnableConfig], *, stop: Optional[List[str]], **kwargs: Any` |\n",
    "| Row 1, Col 1 | `generate` | `messages: List[List[BaseMessage]]` | `LLMResult` | `stop: Optional[List[str]], callbacks: Optional[Union[Callbacks, List[Callbacks]]], *, **kwargs: Any` |\n",
    "| Row 1, Col 1 | `__call__` | `messages: List[BaseMessage]` | `str` | `stop: Optional[List[str]], callbacks: Optional[Union[Callbacks, List[Callbacks]]], **kwargs: Any` |\n",
    "| Row 1, Col 1 | `predict` | `text: str` | `str` | `*, stop: Optional[Sequence[str]], **kwargs: Any` |\n",
    "| Row 1, Col 1 | `predict_messages` | `messages: List[BaseMessage]` | `BaseMessage` | `*, stop: Optional[Sequence[str]], **kwargs: Any` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{Output}^{Input} := \\textcolor{yellow} {BasePromptTemplate_{PromptValue}^{Dict}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**复合运算**\n",
    "\n",
    "$ChatPromptTemplate \\xrightarrow{\\_\\_add\\_\\_(ChatPromptTemplate)} ChatPromptTemplate$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BasePromptTemplate\n",
    "    - StringPromptTemplate\n",
    "        -  PromptTemplate\n",
    "    - BaseChatPromptTemplate\n",
    "        - ChatPromptTemplate\n",
    "            - AgentScratchPadChatPromptTemplate\n",
    "        - AutoGPTPrompt\n",
    "    - PipelinePromptTemplate\n",
    "\n",
    "**运行**\n",
    "\n",
    "| BaesPromptTemplate | Transition | Input | Output | Additional Arguments|\n",
    "|----------|----------|----------|----------|----------|\n",
    "| | `invoke` | `Dict` | `PromptValue` | `config: Optional[RunnableConfig]` |\n",
    "| | `format_prompt` | `**kwargs: Any` | `PromptValue` | |\n",
    "| | global `format_document` | `doc: Document` | `str` | |\n",
    "| BaseChatPromptTemplate|  |  |  | |\n",
    "| | `format` | `**kwargs: Any` | `str` | |\n",
    "| | `format_messages` | `**kwargs: Any` | `List[BaseMessage]` | |\n",
    "| | `format_prompt` | `**kwargs: Any` | `PromptValue` | |\n",
    "| ChatPromptTemplate|  |  |  | |\n",
    "| AgentScratchPadChatPromptTemplate|  |  |  | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构造**\n",
    "\n",
    "| Class | method | Memo |\n",
    "|----------|----------|----------|\n",
    "| ChatPromptTemplate | <li>`input_variables: List[str]`<br> <li>`messages: List[Union[BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate]]` |  |\n",
    "| | `from_template(cls, template: str, **kwargs: Any) → ChatPromptTemplate` | |\n",
    "| | `from_role_strings(cls, string_messages: List[Tuple[str, str]]) → ChatPromptTemplate` | |\n",
    "| | `from_trings(cls, string_messages: List[Tuple[Type[BaseMessagePromptTemplate], str]]) → ChatPromptTemplate` | |\n",
    "| | `from_messages(cls, messages: Sequence[Union[BaseMessagePromptTemplate, BaseChatPromptTemplate, BaseMessage, Tuple[str, str], Tuple[Type, str], str,]]) → ChatPromptTemplate` | |\n",
    "| PipelinePromptTemplate | <li>`final_prompt: BasePromptTemplate`<br> <li>`pipeline_prompts: List[Tuple[str, BasePromptTemplate]]` |  |\n",
    "| AgentScratchPadChatPromptTemplate |  |  |\n",
    "\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{array}{l}\n",
    "        {BaseMessagePromptTemplate} \\\\\n",
    "        {BaseChatPromptTemplate} \\\\\n",
    "        {BaseMessage} \\\\\n",
    "        {Tuple[str, str]} \\\\\n",
    "        {Tuple[Type, str]} \\\\\n",
    "        {str}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\n",
    "\\xrightarrow{\\_convert\\_to\\_message}\n",
    "\n",
    "\\left\\{\n",
    "        \\begin{array}{l}\n",
    "            {BaseMessage} \\\\\n",
    "            {BaseMessagePromptTemplate} \\\\\n",
    "            {BaseChatPromptTemplate}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0() :\n",
    "    llm = OpenAI()\n",
    "    chat = ChatOpenAI()\n",
    "\n",
    "    prettify_json(llm)\n",
    "    binding = llm.bind(stop=['\\n'])\n",
    "    print(binding)\n",
    "\n",
    "    text_input = '写一首四局的古诗，用来赞美无限的宇宙'\n",
    "\n",
    "    # result: LLMResult = llm.generate([text_input])\n",
    "    # print(result)\n",
    "\n",
    "    #result: str = binding.invoke(input=text_input)\n",
    "    #print(_esult)\n",
    "\n",
    "    # coerce example\n",
    "    def raw_prompt_input_str() -> str: return text_input\n",
    "    chain = raw_prompt_input_str | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f1\n",
    "\n",
    "f()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-learning-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
