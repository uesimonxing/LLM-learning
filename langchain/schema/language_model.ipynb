{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input(description: str = \"请输入文本:\") -> str:\n",
    "    text_input = widgets.Text(description=description, layout=widgets.Layout(width=\"100%\"))\n",
    "    display(text_input)\n",
    "    user_input = text_input.value\n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ------ Runnable ------ #\n",
      "invoke, and Output=<class 'langchain.schema.messages.AIMessage'>\n",
      "\tcontent='你是我心中的诗歌，\\n爱情的旋律在你的话语中跳动。\\n你是我眼中的画卷，\\n每一次的相遇都是一幅美丽的画面。\\n\\n你的微笑如春风拂过，\\n温暖了我内心的每一个角落。\\n你的眼神如星辰闪烁，\\n让我沉醉在爱的无尽宇宙。\\n\\n爱是我们心灵的交融，\\n在彼此的怀抱中寻找永恒。\\n你是我心中最美的诗篇，\\n让我们的爱在时间中绽放。\\n\\n无论风雨如何变幻，\\n我们的爱将永远不变。\\n你是我心中的永恒之梦，\\n爱是我们永远的信仰。\\n\\n在这个世界的尽头，\\n我愿与你纵情翱翔。\\n你是我心中的唯一，\\n爱是我们永远的旋律。' additional_kwargs={} example=False\n",
      "RunnableBinding.invoke, and Output=<class 'langchain.schema.messages.AIMessage'>, and temperature=0.7\n",
      "\tcontent='亲爱的AI，倾听我说' additional_kwargs={} example=False\n",
      "True\n",
      "chain.invoke, and Output=<class 'langchain.schema.messages.AIMessage'>\n",
      "\tcontent='在茫茫人海中，我遇见了你，这个存在让我感受到了爱情的美好。爱情的种子在我的心中扎根，绽放出美丽的芬芳。\\n\\n你就像阳光一样，温暖了我寒冷的日子；你就像星辰一样，照亮了我黑暗的夜晚。\\n\\n每一个微笑都是对爱的肯定，每一个拥抱都是心灵的交融。你是我心中永恒的挚爱，让我感受到了爱的力量。\\n\\n爱情就像一首美妙的诗篇，而你是我心中的诗人。每一个字句都流淌着真挚的情感，让我沉浸在爱的海洋中。\\n\\n无论风雨如何变幻，无论岁月如何流转，我愿与你紧紧相拥，共同走过每一个时刻，将这份深深的爱永远珍藏在心中。' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema.runnable import Runnable, RunnableBinding\n",
    "from langchain.schema import *\n",
    "\n",
    "langchain.cache = InMemoryCache()\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "base_chat: BaseChatModel = chat\n",
    "base_language_model: BaseLanguageModel = base_chat\n",
    "runnable: Runnable = base_language_model\n",
    "\n",
    "\n",
    "content: str = \"Hi, I'm Simon from China.\"\n",
    "\n",
    "# ------ BaseChatModel ------ #\n",
    "if False:\n",
    "    print('# ------ BaseChatModel ------ #')\n",
    "    input_message: BaseMessage = HumanMessage(content=content)\n",
    "    output_message: BaseMessage = base_chat(messages=[input_message]) # __call__\n",
    "    print(f'__call__\\n\\t{output_message}')\n",
    "\n",
    "    output_str: str = base_chat.call_as_llm(message=content) # call_as_llm\n",
    "    print(f'call_as_llm\\n\\t{output_str}')\n",
    "\n",
    "# ------ BaseLanguageModel ------ #\n",
    "if False:\n",
    "    print('# ------ BaseLanguageModel ------ #')\n",
    "\n",
    "    user_input: str = input('输入英文句子：')\n",
    "    if user_input == '':\n",
    "        user_input = content\n",
    "\n",
    "    system_message: BaseMessage = SystemMessage(content=\n",
    "        'Please act as a Chinese translator. Translate every sentences user saying to you. Only do the translation, do not add any other words.')\n",
    "    human_message: BaseMessage = HumanMessage(content=user_input)\n",
    "\n",
    "    # predict_messages\n",
    "    output_message: BaseMessage = base_language_model.predict_messages(messages=[system_message, human_message])\n",
    "    print(f'predict_messages\\n\\t{output_message}\\n\\ttokens: {base_language_model.get_num_tokens_from_messages([system_message, human_message, output_message])}')\n",
    "\n",
    "    # generate\n",
    "    another_system_message: BaseMessage = SystemMessage(content=\n",
    "        'Please act as an Engilsh translator. Translate every sentences user saying to you. Only do the translation, do not add any other words.')\n",
    "    another_human_message: BaseMessage = HumanMessage(content=output_message.content)\n",
    "\n",
    "    llm_result: LLMResult = base_language_model.generate(messages=\n",
    "        [\n",
    "            [system_message, human_message],\n",
    "            [another_system_message, another_human_message]\n",
    "        ])\n",
    "\n",
    "    print(f'generate\\n\\t{llm_result}')\n",
    "\n",
    "# ------ Runnable ------ #\n",
    "if True:\n",
    "    print('# ------ Runnable ------ #')\n",
    "\n",
    "    system_message: BaseMessage = SystemMessage(content=\n",
    "        'Please act as a poem writer. write a short poem in Chinese on the topic user saying to you.')\n",
    "    human_message: BaseMessage = HumanMessage(content=input('输入作诗的主题：'))\n",
    "\n",
    "    # invoke, Input=Union[PromptValue, str, List[BaseMessage]]\n",
    "    _input: Union[PromptValue, str, List[BaseMessage]] = [system_message, human_message]\n",
    "    runnable_output = runnable.invoke(input=_input)\n",
    "    print(f'invoke, and Output={type(runnable_output)}\\n\\t{runnable_output}')\n",
    "\n",
    "    # bind.invoke\n",
    "    binding: RunnableBinding = runnable.bind(stop='\\n', temperature=1.0)\n",
    "    bind_output = binding.invoke(input=_input)\n",
    "    print(f'RunnableBinding.invoke, and Output={type(bind_output)}, and temperature={chat.temperature}\\n\\t{bind_output}')\n",
    "    print(binding.bound is chat)\n",
    "\n",
    "    # sequence\n",
    "    def cohere(input: BaseMessage) -> List[BaseMessage]:\n",
    "        \"\"\"see the difference of Input and Output\"\"\"\n",
    "        return [HumanMessage(content=input.content)]\n",
    "\n",
    "    r1 = runnable\n",
    "    r2 = runnable\n",
    "    c = cohere\n",
    "    chain = r1 | c | r2\n",
    "    chain_output = chain.invoke(input=_input)\n",
    "    print(f'chain.invoke, and Output={type(chain_output)}\\n\\t{chain_output}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-learning-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
